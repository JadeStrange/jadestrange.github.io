---
title: 推荐系统中的矩阵分解方法
description: 根据PPT2016 PPT Matrix factorization techniques for recommender systems_slides，总结推荐系统中的矩阵分解方法，包括朴素的矩阵分解，PMF，NMF。
categories:
 - 推荐系统
tags:
 - 推荐系统
 - 矩阵分解
typora-root-url: ..

---

# 推荐系统中的矩阵分解方法



## 1 推荐系统中的矩阵分解方法

该部分是Matrix factorization techniques for recommender systems By Niklas Rowohl的讲解。

### 1.1 使用推荐系统的好处

电子零售商和在线内容提供商(QQ音乐、爱奇艺)用户的选择很多，给User匹配合适的Item可以增加用户粘性。

分析用户兴趣中的模式，从而提供个性化推荐。

### 1.2 内容过滤(卡片)和协同过滤的对比

内容过滤(Content filtering)：用户和商品都有一个卡片(profile)。用户卡片包括年龄、位置等，甚至是一个问卷调查的答案。产品卡片包括描述它的属性，包括题材、演员、发布时间等。使用这些卡片，匹配User/Item。

协同过滤(Collaborative filtering)：依靠过去用户的行为(比如打分、购买)。卡片不必要。而是使用用户和产品的交互关系来明确User/item的联系。

内容过滤的优势：没有冷启动问题。内容过滤事先收集User和Item的数据来生成卡片，协同过滤需要解决冷启动问题，因为新的User/Item开始时与其他User/item没有交互，所以没有任何信息。

协同过滤的优势：

- 领域无关，算法有通用性，视频网站、购物网站都可用。
- 可以解决难以生成卡片的问题。数据过于个性化，难编码。
- 可以用一定的偏向性(biases)/隐式数据(implicit data)来扩展模型等。
- 内容过滤需要user/item的额外信息，可能难以获得。

协同过滤的邻居方法：

1. 围绕用户/物品关系
   - User-User 在一个相似口味的用户群中的高评分Item，被推荐给用户群中的另外的User。
   - Item-Item 某用户喜欢的物品 的相似物品，被推荐给该用户。
2. 隐因子（Latent factor）模型
   - 基于评分的模式，用户和物品被用(20-100)个“特点”来标识。
   - 每个物品和每个用户被用一个向量表示，向量中包含上述“特点”，用户向量有对一个物品某些特点的偏好，物品向量包含他们自身的特点。
   - 用户向量用$p_u$表示，物品向量用$q_i$表示。

### 1.3 基本的矩阵分解

考虑item/user矩阵R包含了用户u给物品i的评分。容易想到，每个用户很难对所有物品都有打分。鉴于此，我们用向量$p_u$和$q_i$估计在R中没被打分的元素。$\hat{R}$中的元素使用下述计算$\hat{r}_{ui}=q_i^T\cdot p_u$。

我们知道了评估矩阵，那我们该如何使用得到向量$p_u/q_i$呢——最小化估计评分和一直评分之间的误差。

最小化loss：
$$
min \sum_{(u,i)\in K}(r_{ui}-q^T_i \cdot p_u)^2+\lambda(\|q_i\|^2+||p_u||^2)
$$
其中K是由明确评分的$r_{ui}$集合，$\lambda$括起来的是正则项。

过拟合是指，模型关注于训练集中的随机误差或者背景噪声。

### 1.4 学习算法

随机梯度下降：初始化随机点，计算梯度，沿梯度反方向找到下一个点，重复过程直到解点足够好。其中学习率可使用固定值，也可使用Armijo算法(https://www.codelast.com/%e5%8e%9f%e5%88%9b%e7%94%a8%e4%ba%ba%e8%af%9d%e8%a7%a3%e9%87%8a%e4%b8%8d%e7%b2%be%e7%a1%ae%e7%ba%bf%e6%90%9c%e7%b4%a2%e4%b8%ad%e7%9a%84armijo-goldstein%e5%87%86%e5%88%99%e5%8f%8awo/)。

交替最小化ALS(alternating least squares)：由于直接用最小二乘不可解，对$p_u/q_i$固定一个用最小二乘法优化另一个。

二者对比：1. ALS可以并行。2. 不稀疏的矩阵，随机梯度下降需要很长时间处理每一个元素，ALS会很快。

### 1.5优化模型

1. 添加Biases，用户的严格程度不同，有的用户苛刻，有的用户宽松。

2. 含蓄隐式的数据，比如购买历史是可以得到用户的进一步的数据。

3. 时空动态，用户的行为、偏好，物品的偏好可能随时间变化。

4. 训练数据的置信度不同，训练数据乘以权重。

### 1.6 结论

1. 内容过滤不用看用户行为，只看用户物品卡片。
2. 协同过滤关注交互关系。
3. 隐银子可以直接的进行矩阵分解，估计评分填充稀疏矩阵。
4. 使用学习算法来估计i/u向量。
5. 隐因子模型易于扩展。

## 2 概率矩阵分解

Probabilistic Matrix Factorization Collaborative-filtering technique for Recommender systems By: Mohamed Emara

PMF模型基于假设——给相似的电影集合评分的用户有相似的偏好。

模型随用户数量线性增长，在庞大的、稀疏的、非常不平衡的Netflix数据集上，表现好。模型强调正则化，来解决所有不适定的问题，或者可以防止任何过拟合。

### 2.1 目前的技术和缺陷

在协同过滤技术有许多不同的方法和算法，有一些基于低维因子。这个是依据与，一个用户的属性、偏好是被小量的未观察的因子决定的。

很多概率因子模型被提出。

使用SVD找最小平方和距离来进行低秩分解。$\dot{\hat{R}}=U^TV$。

缺陷有，表现能力，可伸缩性，难解决稀疏矩阵。除了矩阵分解外，上述提到的方法没有准确的预测。

### 2.2 概率矩阵分解技术

PMF是一个简单的高斯噪声的概率线性模型。

似然函数。
$$
p(R \mid U,V,\sigma^2) = \prod_{i=1}^N \prod_{j=1}^M[p(R_{ij}\mid U_i,V_j,\sigma^2)]
$$

$$
p(R_{ij}\mid U_i,V_j,\sigma^2)=\mathcal N(R_{ij}\mid U_i^TV_j,\sigma^2)
$$
用户和电影的特征向量被给了均值为0的球形高斯先验。
$$
p(U\mid \sigma_U^2)=\prod_{i=1}^N\mathcal N(U_i \mid 0,\sigma_U^2I)
\\
p(V\mid \sigma_V^2)=\prod_{i=1}^N\mathcal N(V_i \mid 0,\sigma_V^2I)
$$

要最大后验概率，将U和V视作类别。其中$I_{ij}$表示用户i是否对物品j打分。
$$
p(U,V|R,\sigma^2)=p(R\mid U,V,\sigma^2)p(U \mid \sigma^2_U)p(V \mid \sigma^2_V)
$$

$$
\ln p(U,V \mid R, \sigma^2) = \sum_{i=1}^N \sum_{j=1}^M I_{ij}lnN(R_{ij}\mid U_i^TV_j,\sigma^2)+\sum_{i=1}^NlnN(U_i\mid 0,\sigma_U^2)+\sum_{j=1}^MlnN(V_j\mid 0,\sigma_V^2)
$$

将高斯分布表达式带入。
$$
L=-\frac{1}{2}(\sum_{i=1}^N\sum_{j=1}^M(R_{ij} - U_i^TV_j)^2_{(i,j)\in已打分集合} + \lambda_U \sum_{i=1}^N\|U_i\|_{Fro}^2 + \lambda_V \sum_{j=1}^M\|V_j\|_{Fro}^2)
$$

$$
\lambda_U=\frac{\sigma^2}{\sigma_U^2}, \lambda_V=\frac{\sigma^2}{\sigma_V^2}
$$

 之后使用EM算法，或者梯度下降，进行对U和V的求解。

除了上述直接使用简单的线性高斯模型直接预测，还可以将预测结果划分到[0, 1]区间，将似然函数改成如下形式。
$$
p(R \mid U, V, \sigma^2) = \prod_{i=1}^N\prod_{j=1}^M[N(R_{ij}\mid g(U_i^TV_j), \sigma^2)]^{I_{ij}}
\\
g(x)=\frac{1}{1 + exp(-x)}
$$
对应的将观测到的Label也从1,...,K映射到[0, 1]区间。
$$
t(x)=\frac{x-1}{K-1}
$$


### 2.3 适应性先验的PMF

为了让模型的泛化性能更好。最简单的方式是改变特征向量的维度来限制模型能力。但又由于数据分布不平衡，无法使用。(比如，有的用户打分多，有的用户打分少，生成的数据集，使得任何单一维度的特征无法适合所有用户、物品，8维度的向量可能对A太短，对B太长。)所以我们使用上文中的超参数$\sigma_U,\sigma_V,\sigma$来控制。调参过程很烦，所以我们直接将这三个超参数放入MAP中。
$$
\ln p(U,V,\sigma^2,\Theta_U,\Theta_V \mid R) = \ln p(R \mid U,V,\sigma^2) + \ln p(U\mid \Theta_U)+\ln p(V \mid \Theta_V) +\ln p(\Theta_U) + \ln p(\Theta_V) + C
$$
可以使用更先进的正则化技术来进行控制，比使用F模进行惩罚更有效。比如使用对角、全协方差矩阵或者自适应的均值，甚至使用高斯混合先验。

对特征使用使用球形高斯先验将会导出$\lambda_U,\lambda_V$自动选择的标准PMF。

这种超参的选择过程比使用验证集手工调参更好。

### 2.4 有约束PMF

提出原因：打分过少的用户的特征向量接近先验均值，或者所有用户的平均向量，导致他们的预测打分接近电影的平均打分。我们发现看过相近电影集合的用户相似程度高，为了个性化上述不活跃的用户，用用户看过的电影对其向量施加强影响。

将$U_i$替换，得到的似然如下所示。
$$
p(R|Y,V,W,\sigma ^2)=\prod_{i=1}^N \prod_{j=1}^M[N(R_{ij}\mid g([Y_i +\frac{\sum_{k=1}^MI_{ik}W_k}{\sum_{k=1}^MI_{ik}} ]^TV_j), \sigma^2)]^{I_{ij}}
$$
每个电影的向量也有一个高斯先验。
$$
p(W\mid \sigma_W) = \prod_{k=1}^MN(W_k \mid 0,\sigma_W^2I)
$$
左图是PMF，右图带约束。

![1](/assets/images/2021-01-07-推荐系统-矩阵分解/1.png)

## 3. 非负矩阵分解

Non-negative Matrix Factorization with Sparseness Constraints By Jan Forkel

$V \rightarrow V\approx WH$

其中W是各特征，H是权重。

![2](/assets/images/2021-01-07-推荐系统-矩阵分解/2.png)![3](/assets/images/2021-01-07-推荐系统-矩阵分解/3.png)

要最小重构误差。
$$
E(W, H) = \|V-WH\|^2=\sum_{i,j}(V_{i,j}-(WH)_{i,j})^2
$$
可以带上稀疏约束。其中一种稀疏的度量方式如下。

![4](/assets/images/2021-01-07-推荐系统-矩阵分解/4.png)

带稀疏约束的NMF如下所示。

![5](/assets/images/2021-01-07-推荐系统-矩阵分解/5.png)

伪代码如下所示。

![6](/assets/images/2021-01-07-推荐系统-矩阵分解/6.png)![7](/assets/images/2021-01-07-推荐系统-矩阵分解/7.png)

伪代码中投影的找到欧氏距离最近的固定一模、二模的非负向量的算法。

![8](/assets/images/2021-01-07-推荐系统-矩阵分解/8.png)